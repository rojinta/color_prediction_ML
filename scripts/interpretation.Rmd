---
title: "PPG Paint Colors: Final Project"
subtitle: "Part iv: Interpretation"
author: "Rojin Taheri"
date: "2023-12-12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load Packages

```{r}
library(tidyverse)
library(caret)
```

## Read Data

```{r}
df <- readr::read_csv("paint_project_train_data.csv", col_names = TRUE)
```

```{r}
df %>% head()
```

```{r}
dfii <- df %>% 
  mutate(y = boot::logit( (response - 0) / (100 - 0) ) ) %>% 
  select(R, G, B, Lightness, Saturation, Hue, y)
dfii %>% head()
```

```{r}
dfiiiD <- df %>% 
  select(-response) %>% 
  mutate(outcome = ifelse(outcome == 1, 'event', 'non_event'),
         outcome = factor(outcome, levels = c('event', 'non_event')))
dfiiiD %>% head()
```

## Part iv: Interpretation  

### ivA: Input Importance  

I identified Model 7 (interaction of the categorical inputs with all main effect and all pairwise interactions of continuous inputs) as the best regression model, and the Random Forest model as the best classification model.  

```{r}
set.seed(2001)

my_ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 5, savePredictions = TRUE)
my_metric <- "RMSE"

mod07 <- train(y ~ (Lightness + Saturation) * (R + G + B + Hue)^2, data = dfii, method = "lm", preProcess = c("center", "scale"), metric = my_metric, trControl = my_ctrl)
```

```{r}
set.seed(123)

my_ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 5, classProbs = TRUE, summaryFunction = twoClassSummary, savePredictions = TRUE)
my_metric <- "ROC"

rf_mod <- train(outcome ~ ., data = dfiiiD, method = "rf", preProcess = c("center", "scale"), trControl = my_ctrl, metric = my_metric)
```

Let's look at the linear regression model first.

```{r}
coef_summary <- summary(mod07$finalModel)

coef_summary$coefficients
```

We will rank and print the coefficients for this model in terms of their estimated value.  

```{r}
model_coef <- coef(mod07$finalModel)

coef_df <- data.frame(Term = names(model_coef), Estimate = model_coef)

coef_df_sorted <- coef_df[order(abs(coef_df$Estimate), decreasing = TRUE), ]

coef_df_sorted
```

It seems like the interactions that include the continuous variables have the larger estimated coefficient values, showing that the continuous inputs are the more important variables to the model.

We will next look at the Random Forest model. Here, we can rank the variables in terms of importance directly.  

```{r}
importance <- varImp(rf_mod, scale = TRUE)

importance
```

Based on the results for the Random Forest model, it seems like the continuous inputs have the highest importance and domination to the model. Hue is the most important variable, and G, R, and then B come respectively after that. The Saturation variable and then the Lightness variable come after the continuous variables in terms of importance.  

In both regression and classification tasks, the continuous variables seem to be most important to the models.  
The continuous input, Hue, is identified as the most important variable, dominating other variables.
Comparing the values for the variable importance ranking for the Random Forest model, the categorical inputs, Lightness and Saturation, seem to be ineffectual by a large margin.  

Let's look at the performance for our classification model, the Random Forest.  

```{r}
max(rf_mod$results$ROC)
```

```{r}
set.seed(123)

my_ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 5, savePredictions = TRUE)
my_metric <- "Accuracy"

rf_mod_acc <- train(outcome ~ ., data = dfiiiD, method = "rf", preProcess = c("center", "scale"), trControl = my_ctrl, metric = my_metric)
```

```{r}
max(rf_mod_acc$results$Accuracy)
```

The Random Forest model has good AUC-ROC and also accuracy values. Based on these results, it seems like the color model inputs are pretty good at identifying popular paints.  

### ivB: Input Insights  

Using the resampled hold-out sets, we need to calculate a performance metric for each prediction compared to the actual values. For regression, we will use the RMSE performance metric, and for classification, we will consider accuracy.  
We will then group the results by combinations of Lightness and Saturation and calculate the average performance metric for each group.  
We will look for combinations with the highest RMSE or lowest accuracy for the hardest to predict combinations, and combinations with the lowest RMSE or highest accuracy for the easiest to predict combinations.  

Let's start with the regression model.  

```{r}
mod07$pred %>% head()
```

```{r}
dfii$rowIndex <- 1:nrow(dfii)

predicted_data <- left_join(mod07$pred, dfii, by = "rowIndex")

predicted_data$squared_error <- (predicted_data$obs - predicted_data$pred)^2

grouped_data <- predicted_data %>%
  group_by(Lightness, Saturation) %>%
  summarize(sum_squared_error = sum(squared_error), n = n()) %>%
  mutate(RMSE = sqrt(sum_squared_error / n))

hardest_to_predict <- grouped_data[which.max(grouped_data$RMSE), ]
easiest_to_predict <- grouped_data[which.min(grouped_data$RMSE), ]
```

```{r}
hardest_to_predict
easiest_to_predict
```

Having the highest RMSE, the `Lightness = dark` and `Saturation = bright` combination is identified as the hardest to predict.  
Having the lowest RMSE, the `Lightness = saturated` and `Saturation = muted` combination is identified as the easiest to predict.  

Now, let's move on to the classification model.  

```{r}
rf_mod$pred %>% head()
```

```{r}
dfiiiD$rowIndex <- 1:nrow(dfiiiD)

predicted_data <- left_join(rf_mod_acc$pred, dfiiiD, by = "rowIndex")

predicted_data$pred_binary <- as.numeric(predicted_data$pred == "event")
predicted_data$obs_binary <- as.numeric(predicted_data$obs == "event")

grouped_data <- predicted_data %>%
  group_by(Lightness, Saturation) %>%
  summarize(
    accuracy = mean(pred_binary == obs_binary)
  )

hardest_to_predict <- grouped_data[which.min(grouped_data$accuracy), ]
easiest_to_predict <- grouped_data[which.max(grouped_data$accuracy), ]
```

```{r}
hardest_to_predict
easiest_to_predict
```

Having the lowest accuracy, the `Lightness = midtone` and `Saturation = neutral` combination is identified as the hardest to predict.  
Having the highest accuracy, the `Lightness = deep` and `Saturation = bright` combination is identified as the easiest to predict.  

### ivC: Prediction Insights  

Visualizing the predictive trends associated with the hardest to predict Lightness and Saturation combinations for the regression model, considering Hue as the primary input, and G as the secondary input:  

```{r}
mean_R <- mean(df$R)
mean_B <- mean(df$B)
light_value <- "dark"
satur_value <- "bright"

prediction_grid <- expand.grid(Hue = seq(min(df$Hue), max(df$Hue), length.out = 101),
                               G = seq(min(df$G), max(df$G), length.out = 101),
                               Lightness = light_value,
                               Saturation = satur_value,
                               R = mean_R,
                               B = mean_B)

prediction_grid$pred_mod07 <- predict(mod07, newdata = prediction_grid)

prediction_grid %>%
  ggplot(mapping = aes(x = Hue, y = G)) +
  geom_raster(mapping = aes(fill = pred_mod07)) +
  scale_fill_gradientn(colours = terrain.colors(10)) +
  theme_minimal() +
  labs(title = "Hardest to Predict Combination for Regression Model", x = "Hue", y = "G", fill = "y")
```

Visualizing the predictive trends associated with the easiest to predict Lightness and Saturation combinations for the regression model, considering Hue as the primary input, and G as the secondary input:

```{r}
mean_R <- mean(df$R)
mean_B <- mean(df$B)
light_value <- "saturated"
satur_value <- "muted"

prediction_grid <- expand.grid(Hue = seq(min(df$Hue), max(df$Hue), length.out = 101),
                               G = seq(min(df$G), max(df$G), length.out = 101),
                               Lightness = light_value,
                               Saturation = satur_value,
                               R = mean_R,
                               B = mean_B)

prediction_grid$pred_mod07 <- predict(mod07, newdata = prediction_grid)

prediction_grid %>%
  ggplot(mapping = aes(x = Hue, y = G)) +
  geom_raster(mapping = aes(fill = pred_mod07)) +
  scale_fill_gradientn(colours = terrain.colors(10)) +
  theme_minimal() +
  labs(title = "Easiest to Predict Combination for Regression Model", x = "Hue", y = "G", fill = "y")
```

Visualizing the predictive trends associated with the hardest to predict Lightness and Saturation combinations for the classification model, considering Hue as the primary input, and G as the secondary input:

```{r}
mean_R <- mean(df$R)
mean_B <- mean(df$B)
light_value <- "midtone"
satur_value <- "neutral"

prediction_grid <- expand.grid(Hue = seq(min(df$Hue), max(df$Hue), length.out = 101),
                               G = seq(min(df$G), max(df$G), length.out = 101),
                               Lightness = light_value,
                               Saturation = satur_value,
                               R = mean_R,
                               B = mean_B)

prediction_grid$pred_rf_mod_acc <- predict(rf_mod_acc, newdata = prediction_grid, type = "prob")

prediction_grid$event_prob <- prediction_grid$pred_rf_mod_acc[, "event"]

prediction_grid %>%
  ggplot(mapping = aes(x = Hue, y = G)) +
  geom_raster(mapping = aes(fill = event_prob)) +
  scale_fill_gradientn(colours = terrain.colors(10)) +
  theme_minimal() +
  labs(title = "Hardest to Predict Combination for Classification Model", x = "Hue", y = "G", fill = "Event Probability")
```

Visualizing the predictive trends associated with the easiest to predict Lightness and Saturation combinations for the classification model, considering Hue as the primary input, and G as the secondary input:

```{r}
mean_R <- mean(df$R)
mean_B <- mean(df$B)
light_value <- "deep"
satur_value <- "bright"

prediction_grid <- expand.grid(Hue = seq(min(df$Hue), max(df$Hue), length.out = 101),
                               G = seq(min(df$G), max(df$G), length.out = 101),
                               Lightness = light_value,
                               Saturation = satur_value,
                               R = mean_R,
                               B = mean_B)

prediction_grid$pred_rf_mod_acc <- predict(rf_mod_acc, newdata = prediction_grid, type = "prob")

prediction_grid$event_prob <- prediction_grid$pred_rf_mod_acc[, "event"]

prediction_grid %>%
  ggplot(mapping = aes(x = Hue, y = G)) +
  geom_raster(mapping = aes(fill = event_prob)) +
  scale_fill_gradientn(colours = terrain.colors(10)) +
  theme_minimal() +
  labs(title = "Easiest to Predict Combination for Classification Model", x = "Hue", y = "G", fill = "Event Probability")
```

The surface plots for the hardest and easiest to predict combinations for the regression model look, more or less, the same. The `x` axis, Hue, appears to have a lesser effect on the predictions compared to the `y` axis, G, which shows a clearer gradient from one end to the other. This suggests that G might be a more influential predictor in this model. The easiest to predict plot indicates a more stable model behavior and a clearer relationship between Hue and G.  

The trends associated with the hardest and easiest to predict combinations for the classification model are pretty much different. The easiest to predict plot shows a much more consistent color throughout most of the area, indicating that the model gives similar event probabilities across a wide range of Hue and G values. The hardest to predict plot has much more variability and distinct patches of different colors, which shows more areas of uncertainty in the model's prediction. In the easiest to predict plot, the event probabilities appear to be higher overall.

