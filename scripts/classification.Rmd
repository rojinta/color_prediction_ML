---
title: "PPG Paint Colors: Final Project"
subtitle: "Part iii: Classification"
author: "Rojin Taheri"
date: "2023-12-12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load Packages

```{r}
library(tidyverse)
library(caret)
```

## Read Data

```{r}
df <- readr::read_csv("paint_project_train_data.csv", col_names = TRUE)
```

```{r}
df %>% head()
```

Before anything, we must pay attention to the empirical proportion of the `event`!

```{r}
mean(df$outcome == 1)
```

## Part iii: Classification  

### iiiA: Generalized Linear Models  

```{r}
dfiiiA <- df %>% 
  select(-response)
dfiiiA %>% head()
```

```{r}
mod01 <- glm(outcome ~ 1, data = dfiiiA, family = binomial)
mod02 <- glm(outcome ~ Lightness + Saturation, data = dfiiiA, family = binomial)
mod03 <- glm(outcome ~ R + G + B + Hue, data = dfiiiA, family = binomial)
mod04 <- glm(outcome ~ R + G + B + Hue + Lightness + Saturation, data = dfiiiA, family = binomial)
mod05 <- glm(outcome ~ (Lightness + Saturation) * (R + G + B + Hue), data = dfiiiA, binomial)
mod06 <- glm(outcome ~ Lightness + Saturation + (R + G + B + Hue)^2, data = dfiiiA, family = binomial)
mod07 <- glm(outcome ~ (Lightness + Saturation) * (R + G + B + Hue)^2, data = dfiiiA, family = binomial)
mod08 <- glm(outcome ~ I(R^2) + I(G^2) + I(B) + I(Hue^2), data = dfiiiA, family = binomial)
mod09 <- glm(outcome ~ I(R^2) * I(G^2) * I(B) * I(Hue^2), data = dfiiiA, family = binomial)
mod10 <- glm(outcome ~ (Lightness + Saturation) * (I(R^2) + I(G^2) + I(B) + I(Hue^2)), data = dfiiiA, family = binomial)
```

While fitting the generalized linear models, I got a warning message as follows: "Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred". This can happen in logistic regression when the model is trying to predict a binary outcome and the model is very certain, or very uncertain about certain predictions. It can both be a sign of perfect separation or overfitting.  

I will, next, be using the AUC-ROC performance metric to evaluate and compare the 10 models.  

```{r}
library(pROC)
```

```{r}
predict_mod01 <- predict(mod01, dfiiiA, type = "response")
roc_mod01 <- roc(df$outcome, predict_mod01)
auc_mod01 <- auc(roc_mod01)

predict_mod02 <- predict(mod02, dfiiiA, type = "response")
roc_mod02 <- roc(df$outcome, predict_mod02)
auc_mod02 <- auc(roc_mod02)

predict_mod03 <- predict(mod03, dfiiiA, type = "response")
roc_mod03 <- roc(df$outcome, predict_mod03)
auc_mod03 <- auc(roc_mod03)

predict_mod04 <- predict(mod04, dfiiiA, type = "response")
roc_mod04 <- roc(df$outcome, predict_mod04)
auc_mod04 <- auc(roc_mod04)

predict_mod05 <- predict(mod05, dfiiiA, type = "response")
roc_mod05 <- roc(df$outcome, predict_mod05)
auc_mod05 <- auc(roc_mod05)

predict_mod06 <- predict(mod06, dfiiiA, type = "response")
roc_mod06 <- roc(df$outcome, predict_mod06)
auc_mod06 <- auc(roc_mod06)

predict_mod07 <- predict(mod07, dfiiiA, type = "response")
roc_mod07 <- roc(df$outcome, predict_mod07)
auc_mod07 <- auc(roc_mod07)

predict_mod08 <- predict(mod08, dfiiiA, type = "response")
roc_mod08 <- roc(df$outcome, predict_mod08)
auc_mod08 <- auc(roc_mod08)

predict_mod09 <- predict(mod09, dfiiiA, type = "response")
roc_mod09 <- roc(df$outcome, predict_mod09)
auc_mod09 <- auc(roc_mod09)

predict_mod10 <- predict(mod10, dfiiiA, type = "response")
roc_mod10 <- roc(df$outcome, predict_mod10)
auc_mod10 <- auc(roc_mod10)

model_names <- c("Model 1", "Model 2", "Model 3", "Model 4", "Model 5", "Model 6", "Model 7", "Model 8", "Model 9", "Model 10")

auc_values <- c(auc_mod01, auc_mod02, auc_mod03, auc_mod04, auc_mod05, auc_mod06, auc_mod07, auc_mod08, auc_mod09, auc_mod10)

model_auc <- data.frame(Models = model_names, AUC = auc_values)
model_auc
```

Model 7 is the best performing model, having the highest AUC-ROC value. Model 5 and 10 are the second and third top models.  

```{r}
coefplot::coefplot(mod07)
coefplot::coefplot(mod05)
coefplot::coefplot(mod10)
```

In all of the 3 models, a majority of the coefficient values are zero, and only a few number of the coefficients seem important to the models.  

### iiiB: Bayesian Generalized Linear Models  

I am choosing Model 7 and Model 5, as Model 5 also has a high value of AUC-ROC, but is less complex than Model 7.

```{r}
library(rstanarm)
```

```{r}
mod07_bayesian <- stan_glm(outcome ~ (Lightness + Saturation) * (R + G + B + Hue)^2, data = dfiiiA, family = binomial)
mod05_bayesian <- stan_glm(outcome ~ (Lightness + Saturation) * (R + G + B + Hue), data = df, family = binomial)
```

I will use the leave-one-out cross validation to evaluate the 2 models' performances and select the best of the 2.  

```{r}
library(loo)
```

```{r}
loo(mod07_bayesian)
```

```{r}
loo(mod05_bayesian)
```

```{r}
loo_compare(loo(mod07_bayesian), loo(mod05_bayesian))
```

Having the lower LOO-CV values, Model 7 is considered the best of these 2 models.  

```{r}
plot(mod07_bayesian)
```

### iiiC: Generalized Linear Models Predictions  

We will start with visualizing the predictive trends for Model 7.  

Choosing R as the primary input and G as the secondary input, keeping B and Hue at a reference value of their mean, and Lightness and Saturation as, respectively, "midtone" and "neutral":  

```{r}
mean_B <- mean(df$B)
mean_Hue <- mean(df$Hue)

prediction_data <- expand.grid(R = seq(min(df$R), max(df$R), length.out = 100),
                               G = seq(min(df$G), max(df$G), length.out = 6),
                               Lightness = "midtone",
                               Saturation = "neutral",
                               B = mean_B,
                               Hue = mean_Hue)

prediction_data$pred_mod07 <- predict(mod07, prediction_data, type = "response")

pred_se <- predict(mod07, prediction_data, type = "response", se.fit = TRUE)

ci_width <- qnorm(0.975) * pred_se$se.fit
prediction_data$lower_conf <- prediction_data$pred_mod07 - ci_width
prediction_data$upper_conf <- prediction_data$pred_mod07 + ci_width

prediction_data %>%
  ggplot(mappin = aes(x = R, y = pred_mod07)) +
  geom_line() +
  facet_wrap(~ G) +
  geom_ribbon(aes(ymin = lower_conf, ymax = upper_conf), alpha = 0.5, fill = 'grey') +
  labs(title = "Primary Input = R, Secondary Input = G", x = "R", y = "Predicted Event Probability")
```

Choosing R as the primary input and B as the secondary input, keeping G and Hue at a reference value of their mean, and Lightness and Saturation as, respectively, "midtone" and "neutral":  

```{r}
mean_G <- mean(df$G)
mean_Hue <- mean(df$Hue)

prediction_data <- expand.grid(R = seq(min(df$R), max(df$R), length.out = 100),
                               B = seq(min(df$B), max(df$B), length.out = 6),
                               Lightness = "midtone",
                               Saturation = "neutral",
                               G = mean_G,
                               Hue = mean_Hue)

prediction_data$pred_mod07 <- predict(mod07, prediction_data, type = "response")

pred_se <- predict(mod07, prediction_data, type = "response", se.fit = TRUE)

ci_width <- qnorm(0.975) * pred_se$se.fit
prediction_data$lower_conf <- prediction_data$pred_mod07 - ci_width
prediction_data$upper_conf <- prediction_data$pred_mod07 + ci_width

prediction_data %>%
  ggplot(mappin = aes(x = R, y = pred_mod07)) +
  geom_line() +
  facet_wrap(~ B) +
  geom_ribbon(aes(ymin = lower_conf, ymax = upper_conf), alpha = 0.5, fill = 'grey') +
  labs(title = "Primary Input = R, Secondary Input = B", x = "R", y = "Predicted Event Probability")
```

Choosing R as the primary input and Hue as the secondary input, keeping G and B at a reference value of their mean, and Lightness and Saturation as, respectively, "midtone" and "neutral":  

```{r}
mean_G <- mean(df$G)
mean_B <- mean(df$B)

prediction_data <- expand.grid(R = seq(min(df$R), max(df$R), length.out = 100),
                               Hue = seq(min(df$Hue), max(df$Hue), length.out = 6),
                               Lightness = "midtone",
                               Saturation = "neutral",
                               G = mean_G,
                               B = mean_B)

prediction_data$pred_mod07 <- predict(mod07, prediction_data, type = "response")

pred_se <- predict(mod07, prediction_data, type = "response", se.fit = TRUE)

ci_width <- qnorm(0.975) * pred_se$se.fit
prediction_data$lower_conf <- prediction_data$pred_mod07 - ci_width
prediction_data$upper_conf <- prediction_data$pred_mod07 + ci_width

prediction_data %>%
  ggplot(mappin = aes(x = R, y = pred_mod07)) +
  geom_line() +
  facet_wrap(~ Hue) +
  geom_ribbon(aes(ymin = lower_conf, ymax = upper_conf), alpha = 0.5, fill = 'grey') +
  labs(title = "Primary Input = R, Secondary Input = Hue", x = "R", y = "Predicted Event Probability")
```

We will then proceed to visualize the predictive trends for Model 5.  

Choosing R as the primary input and G as the secondary input, keeping B and Hue at a reference value of their mean, and Lightness and Saturation as, respectively, "midtone" and "neutral":  

```{r}
mean_B <- mean(df$B)
mean_Hue <- mean(df$Hue)

prediction_data <- expand.grid(R = seq(min(df$R), max(df$R), length.out = 100),
                               G = seq(min(df$G), max(df$G), length.out = 6),
                               Lightness = "midtone",
                               Saturation = "neutral",
                               B = mean_B,
                               Hue = mean_Hue)

prediction_data$pred_mod05 <- predict(mod05, prediction_data, type = "response")

pred_se <- predict(mod05, prediction_data, type = "response", se.fit = TRUE)

ci_width <- qnorm(0.975) * pred_se$se.fit
prediction_data$lower_conf <- prediction_data$pred_mod05 - ci_width
prediction_data$upper_conf <- prediction_data$pred_mod05 + ci_width

prediction_data %>%
  ggplot(mappin = aes(x = R, y = pred_mod05)) +
  geom_line() +
  facet_wrap(~ G) +
  geom_ribbon(aes(ymin = lower_conf, ymax = upper_conf), alpha = 0.5, fill = 'grey') +
  labs(title = "Primary Input = R, Secondary Input = G", x = "R", y = "Predicted Event Probability")
```

Choosing R as the primary input and B as the secondary input, keeping G and Hue at a reference value of their mean, and Lightness and Saturation as, respectively, "midtone" and "neutral":  

```{r}
mean_G <- mean(df$G)
mean_Hue <- mean(df$Hue)

prediction_data <- expand.grid(R = seq(min(df$R), max(df$R), length.out = 100),
                               B = seq(min(df$B), max(df$B), length.out = 6),
                               Lightness = "midtone",
                               Saturation = "neutral",
                               G = mean_G,
                               Hue = mean_Hue)

prediction_data$pred_mod05 <- predict(mod05, prediction_data, type = "response")

pred_se <- predict(mod05, prediction_data, type = "response", se.fit = TRUE)

ci_width <- qnorm(0.975) * pred_se$se.fit
prediction_data$lower_conf <- prediction_data$pred_mod05 - ci_width
prediction_data$upper_conf <- prediction_data$pred_mod05 + ci_width

prediction_data %>%
  ggplot(mappin = aes(x = R, y = pred_mod05)) +
  geom_line() +
  facet_wrap(~ B) +
  geom_ribbon(aes(ymin = lower_conf, ymax = upper_conf), alpha = 0.5, fill = 'grey') +
  labs(title = "Primary Input = R, Secondary Input = G", x = "R", y = "Predicted Event Probability")
```

Choosing R as the primary input and Hue as the secondary input, keeping G and B at a reference value of their mean, and Lightness and Saturation as, respectively, "midtone" and "neutral":  

```{r}
mean_G <- mean(df$G)
mean_B <- mean(df$B)

prediction_data <- expand.grid(R = seq(min(df$R), max(df$R), length.out = 100),
                               Hue = seq(min(df$Hue), max(df$Hue), length.out = 6),
                               Lightness = "midtone",
                               Saturation = "neutral",
                               G = mean_G,
                               B = mean_B)

prediction_data$pred_mod05 <- predict(mod05, prediction_data, type = "response")

pred_se <- predict(mod05, prediction_data, type = "response", se.fit = TRUE)

ci_width <- qnorm(0.975) * pred_se$se.fit
prediction_data$lower_conf <- prediction_data$pred_mod05 - ci_width
prediction_data$upper_conf <- prediction_data$pred_mod05 + ci_width

prediction_data %>%
  ggplot(mappin = aes(x = R, y = pred_mod05)) +
  geom_line() +
  facet_wrap(~ Hue) +
  geom_ribbon(aes(ymin = lower_conf, ymax = upper_conf), alpha = 0.5, fill = 'grey') +
  labs(title = "Primary Input = R, Secondary Input = G", x = "R", y = "Predicted Event Probability")
```

The predictive trends for the 2 generalized linear models are similar. Overall, Model 5 has larger confidence intervals compared to Model 7.  

### iiiD: Train/Tune with Resampling  

```{r}
dfiiiD <- df %>% 
  select(-response) %>% 
  mutate(outcome = ifelse(outcome == 1, 'event', 'non_event'),
         outcome = factor(outcome, levels = c('event', 'non_event')))
dfiiiD %>% head()
```

```{r}
library(glmnet)
library(nnet)
library(randomForest)
library(gbm)
library(e1071)
library(earth)
```

```{r}
set.seed(123)

my_ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 5, classProbs = TRUE, summaryFunction = twoClassSummary, savePredictions = TRUE)
my_metric <- "ROC"
 
## Linear models
linear_mod01 <- train(outcome ~ ., data = dfiiiD, method = "glm", family = binomial, preProcess = c("center", "scale"), trControl = my_ctrl, metric = my_metric)

linear_mod02 <- train(outcome ~ Lightness + Saturation + (R + G + B + Hue)^2, data = dfiiiD, method = "glm", family = binomial, preProcess = c("center", "scale"), trControl = my_ctrl, metric = my_metric)

linear_mod03 <- train(outcome ~ (Lightness + Saturation) * (R + G + B + Hue), data = dfiiiD, method = "glm", family = binomial, preProcess = c("center", "scale"), trControl = my_ctrl, metric = my_metric)

linear_mod04 <- train(outcome ~ (Lightness + Saturation) * (R + G + B + Hue)^2, data = dfiiiD, method = "glm", family = binomial, preProcess = c("center", "scale"), trControl = my_ctrl, metric = my_metric)

## Regularized regression with elastic net
enet_tune_grid <- expand.grid(alpha = seq(0, 1, length = 5), lambda = seq(0.001, 0.1, length = 5))

enet_mod01 <- train(outcome ~ Lightness + Saturation + (R + G + B + Hue)^2, data = dfiiiD, method = "glmnet", preProcess = c("center", "scale"), trControl = my_ctrl, metric = my_metric, tuneGrid = enet_tune_grid)

enet_mod02 <- train(outcome ~ (Lightness + Saturation) * (R + G + B + Hue)^2, data = dfiiiD, method = "glmnet", preProcess = c("center", "scale"), trControl = my_ctrl, metric = my_metric, tuneGrid = enet_tune_grid)

## Neural network
nn_mod <- train(outcome ~ ., data = dfiiiD, method = "nnet", preProcess = c("center", "scale"), trControl = my_ctrl, metric = my_metric)
  
## Random forest
rf_mod <- train(outcome ~ ., data = dfiiiD, method = "rf", preProcess = c("center", "scale"), trControl = my_ctrl, metric = my_metric)
  
## Gradient boosted tree
gbm_mod <- train(outcome ~ ., data = dfiiiD, method = "gbm", preProcess = c("center", "scale"), trControl = my_ctrl, metric = my_metric)
  
## Support vector machines
svm_mod <- train(outcome ~ ., data = dfiiiD, method = "svmRadial", preProcess = c("center", "scale"), trControl = my_ctrl, metric = my_metric)
  
## Multivariate additive regression splines
mars_mod <- train(outcome ~ ., data = dfiiiD, method = "earth", preProcess = c("center", "scale"), trControl = my_ctrl, metric = my_metric)
```

I will be using the AUC-ROC performance metric to evaluate the models and identify the best performing model.  

```{r}
linear_mod01_auc <- linear_mod01$results$ROC
linear_mod02_auc <- linear_mod02$results$ROC
linear_mod03_auc <- linear_mod03$results$ROC
linear_mod04_auc <- linear_mod04$results$ROC
enet_mod01_auc <- max(enet_mod01$results$ROC)
enet_mod02_auc <- max(enet_mod02$results$ROC)
nn_mod_auc <- max(nn_mod$results$ROC)
rf_mod_auc <- max(rf_mod$results$ROC)
gbm_mod_auc <- max(gbm_mod$results$ROC)
svm_mod_auc <- max(svm_mod$results$ROC)
mars_mod_auc <- max(mars_mod$results$ROC)

model_names <- c("Linear 1", "Linear 2", "Linear 3", "Linear 4", "ENet 1", "ENet 2", "NN", "RF", "GBM", "SVM", "MARS")

auc_values <- c(linear_mod01_auc, linear_mod02_auc, linear_mod03_auc, linear_mod04_auc, enet_mod01_auc, enet_mod02_auc, nn_mod_auc, rf_mod_auc, gbm_mod_auc, svm_mod_auc, mars_mod_auc)

model_auc <- data.frame(Model = model_names, AUC = auc_values)
model_auc
```

Having the highest AUC-ROC value, the Random Forest model is identified as the best performing model.

Next, we will use the accuracy performance metric to evaluate the models, and look for any potential differences in the cases of the 2 metrics.  

```{r}
set.seed(123)

my_ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 5)
my_metric <- "Accuracy"
 
## Linear models
linear_mod01 <- train(outcome ~ ., data = dfiiiD, method = "glm", family = binomial, preProcess = c("center", "scale"), trControl = my_ctrl, metric = my_metric)

linear_mod02 <- train(outcome ~ Lightness + Saturation + (R + G + B + Hue)^2, data = dfiiiD, method = "glm", family = binomial, preProcess = c("center", "scale"), trControl = my_ctrl, metric = my_metric)

linear_mod03 <- train(outcome ~ (Lightness + Saturation) * (R + G + B + Hue), data = dfiiiD, method = "glm", family = binomial, preProcess = c("center", "scale"), trControl = my_ctrl, metric = my_metric)

linear_mod04 <- train(outcome ~ (Lightness + Saturation) * (R + G + B + Hue)^2, data = dfiiiD, method = "glm", family = binomial, preProcess = c("center", "scale"), trControl = my_ctrl, metric = my_metric)

## Regularized regression with elastic net
enet_tune_grid <- expand.grid(alpha = seq(0, 1, length = 5), lambda = seq(0.001, 0.1, length = 5))

enet_mod01 <- train(outcome ~ Lightness + Saturation + (R + G + B + Hue)^2, data = dfiiiD, method = "glmnet", preProcess = c("center", "scale"), trControl = my_ctrl, metric = my_metric, tuneGrid = enet_tune_grid)

enet_mod02 <- train(outcome ~ (Lightness + Saturation) * (R + G + B + Hue)^2, data = dfiiiD, method = "glmnet", preProcess = c("center", "scale"), trControl = my_ctrl, metric = my_metric, tuneGrid = enet_tune_grid)

## Neural network
nn_mod <- train(outcome ~ ., data = dfiiiD, method = "nnet", preProcess = c("center", "scale"), trControl = my_ctrl, metric = my_metric)
  
## Random forest
rf_mod <- train(outcome ~ ., data = dfiiiD, method = "rf", preProcess = c("center", "scale"), trControl = my_ctrl, metric = my_metric)
  
## Gradient boosted tree
gbm_mod <- train(outcome ~ ., data = dfiiiD, method = "gbm", preProcess = c("center", "scale"), trControl = my_ctrl, metric = my_metric)
  
## Support vector machines
svm_mod <- train(outcome ~ ., data = dfiiiD, method = "svmRadial", preProcess = c("center", "scale"), trControl = my_ctrl, metric = my_metric)
  
## Multivariate additive regression splines
mars_mod <- train(outcome ~ ., data = dfiiiD, method = "earth", preProcess = c("center", "scale"), trControl = my_ctrl, metric = my_metric)
```

```{r}
linear_mod01_acc <- linear_mod01$results$Accuracy
linear_mod02_acc <- linear_mod02$results$Accuracy
linear_mod03_acc <- linear_mod03$results$Accuracy
linear_mod04_acc <- linear_mod04$results$Accuracy
enet_mod01_acc <- max(enet_mod01$results$Accuracy)
enet_mod02_acc <- max(enet_mod02$results$Accuracy)
nn_mod_acc <- max(nn_mod$results$Accuracy)
rf_mod_acc <- max(rf_mod$results$Accuracy)
gbm_mod_acc <- max(gbm_mod$results$Accuracy)
svm_mod_acc <- max(svm_mod$results$Accuracy)
mars_mod_acc <- max(mars_mod$results$Accuracy)

acc_values <- c(linear_mod01_acc, linear_mod02_acc, linear_mod03_acc, linear_mod04_acc, enet_mod01_acc, enet_mod02_acc, nn_mod_acc, rf_mod_acc, gbm_mod_acc, svm_mod_acc, mars_mod_acc)

model_acc <- data.frame(Model = model_names, Accuracy = acc_values)
model_acc
```

Based on the results for the models' accuracy values, the Random Forest model is again identified as the best performing model, because it has the highest possible accuracy value!  

